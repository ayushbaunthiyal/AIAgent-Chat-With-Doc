# RAG Chat Assistant

A conversational AI assistant that answers questions about your documents using **RAG (Retrieval-Augmented Generation)** with a **LangGraph ReAct agent**, **MCP Server**, and **Chroma vector database**.

![Python](https://img.shields.io/badge/Python-3.10--3.13-blue)
![LangGraph](https://img.shields.io/badge/LangGraph-ReAct_Agent-green)
![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4_Turbo-orange)
![Docker](https://img.shields.io/badge/Docker-Ready-blue)

---

## üìã Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Technical Design](#-technical-design)
- [Quick Start - Local Setup](#-quick-start---local-setup)
- [Quick Start - Docker Setup](#-quick-start---docker-setup)
- [Configuration](#-configuration)
- [Project Structure](#-project-structure)
- [RAG/LLM Approach & Decisions](#-ragllm-approach--decisions)
- [Production Considerations](#-production-considerations)
- [Testing](#-testing)
- [Troubleshooting](#-troubleshooting)

---

## ‚ú® Features

| Feature | Description |
|---------|-------------|
| **Document Upload** | Support for PDF, TXT, and Markdown files |
| **Intelligent Chunking** | Recursive character splitting (1000 chars, 200 overlap) |
| **Semantic Search** | Vector-based similarity search using Chroma |
| **ReAct Agent** | LangGraph-powered agent with multi-step reasoning |
| **MCP Integration** | Tool-based document access via Model Context Protocol |
| **Persistent Storage** | Chroma vector store with local file persistence |
| **Session Memory** | In-memory conversation history per session |
| **Collection Management** | Clear all documents with one click |
| **Source Citations** | Answers include references to source chunks |

---

## üèóÔ∏è Architecture

### High-Level Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              STREAMLIT UI                                    ‚îÇ
‚îÇ                         (Document Upload + Chat)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          LANGGRAPH REACT AGENT                               ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ  THINK   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   ACT    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ OBSERVE  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ GENERATE ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ(Reason)  ‚îÇ    ‚îÇ(Use Tool)‚îÇ    ‚îÇ(Process) ‚îÇ    ‚îÇ(Respond) ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ        ‚ñ≤                                              ‚îÇ                      ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ                      (Loop until answer ready)                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº                                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        MCP SERVER             ‚îÇ     ‚îÇ            OPENAI API                  ‚îÇ
‚îÇ  (Model Context Protocol)     ‚îÇ     ‚îÇ                                        ‚îÇ
‚îÇ                               ‚îÇ     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  Tools:                       ‚îÇ     ‚îÇ  ‚îÇ GPT-4 Turbo ‚îÇ  ‚îÇ text-embedding- ‚îÇ ‚îÇ
‚îÇ  ‚Ä¢ search_documents           ‚îÇ     ‚îÇ  ‚îÇ   (LLM)     ‚îÇ  ‚îÇ   3-small       ‚îÇ ‚îÇ
‚îÇ  ‚Ä¢ get_document_chunk         ‚îÇ     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚Ä¢ list_documents             ‚îÇ     ‚îÇ                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CHROMA VECTOR DATABASE                               ‚îÇ
‚îÇ                        (Local Persistent Storage)                            ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ  Collection: "documents"                                             ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  ‚îú‚îÄ‚îÄ Document Chunks (text content)                                  ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  ‚îú‚îÄ‚îÄ Embeddings (1536-dim vectors)                                   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ Metadata (source, chunk_id, page_number)                        ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ   Storage: ./data/chroma_db/ (SQLite + HNSW index)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

```
1. DOCUMENT INGESTION
   Upload PDF/TXT ‚Üí Chunking (1000 chars) ‚Üí Embedding ‚Üí Store in Chroma

2. QUERY PROCESSING
   User Query ‚Üí ReAct Agent ‚Üí MCP Tools ‚Üí Chroma Search ‚Üí Retrieve Chunks

3. RESPONSE GENERATION
   Retrieved Chunks ‚Üí Context Assembly ‚Üí GPT-4 ‚Üí Answer with Citations
```

### Component Interactions

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Streamlit    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  LangGraph     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    OpenAI      ‚îÇ
‚îÇ   (app.py)     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  ReAct Agent   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ    LLM API     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                       ‚îÇ
        ‚îÇ                       ‚îÇ Tool Calls
        ‚îÇ                       ‚ñº
        ‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ               ‚îÇ   MCP Server   ‚îÇ
        ‚îÇ               ‚îÇ   (stdio)      ‚îÇ
        ‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                       ‚îÇ
        ‚îÇ                       ‚ñº
        ‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Chroma      ‚îÇ
        (Direct access) ‚îÇ  Vector Store  ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Technical Design

### Core Components

| Component | File | Purpose |
|-----------|------|---------|
| **Config** | `src/config.py` | Pydantic settings, environment variable management |
| **Document Processor** | `src/document_processor.py` | PDF/TXT loading, recursive chunking |
| **Embeddings** | `src/embeddings.py` | OpenAI embedding generation wrapper |
| **Vector Store** | `src/vector_store.py` | Chroma operations (add, search, delete) |
| **MCP Server** | `src/mcp_server/` | Tool definitions, server setup |
| **MCP Client** | `src/mcp_client.py` | MCP adapter for LangGraph |
| **Retrieval** | `src/retrieval.py` | Hybrid retrieval (MCP + vector search) |
| **Agent** | `src/agent.py` | LangGraph ReAct agent implementation |
| **Prompts** | `src/prompts.py` | System prompts and templates |
| **App** | `app.py` | Streamlit web interface |

### Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| **Agent Framework** | LangGraph ReAct | Built-in reasoning loop, state management, tool integration |
| **Vector Database** | Chroma (local) | Zero infrastructure cost, persistent, easy setup |
| **LLM** | GPT-4-turbo | Best reasoning for ReAct, high-quality responses |
| **Embeddings** | text-embedding-3-small | Cost-effective, 1536 dimensions, good quality |
| **Context Protocol** | MCP (stdio) | Standardized tool access, LangChain adapters available |
| **Memory** | Session-based (in-memory) | Simple, clears on refresh, no database needed |
| **Chunking** | RecursiveCharacterTextSplitter | Preserves context, configurable overlap |

### State Management

```python
# LangGraph Agent State
{
    "messages": List[BaseMessage],      # Conversation history (trimmed to last 10)
    "context": str,                      # Retrieved document chunks
    "iteration_count": int,              # ReAct loop counter
    "final_response": str                # Generated answer
}

# Streamlit Session State
{
    "messages": List[dict],              # Chat display history
    "agent": RAGAgent,                   # Agent instance
    "vector_store": VectorStore,         # Chroma wrapper
    "embedding_service": EmbeddingService,
    "document_processor": DocumentProcessor
}
```

### MCP Tools

| Tool | Description | Parameters |
|------|-------------|------------|
| `search_documents` | Semantic search across all documents | `query: str`, `top_k: int` |
| `get_document_chunk` | Retrieve specific chunk by ID | `chunk_id: str` |
| `list_documents` | List all stored document sources | None |

---

## üöÄ Quick Start - Local Setup

### Prerequisites

- **Python 3.10-3.13** (3.13 recommended)
- **OpenAI API Key** ([Get one here](https://platform.openai.com/api-keys))
- **Git** (for cloning)

### Step 1: Clone the Repository

```bash
git clone https://github.com/ayush-baunthiyal/AIAgent-Chat-With-Doc.git
cd AIAgent-Chat-With-Doc
```

### Step 2: Create Virtual Environment

**Option A: Using UV (Recommended - Faster)**
```bash
# Install UV if not already installed
pip install uv

# Create virtual environment
uv venv

# Activate virtual environment
# Windows PowerShell:
.\.venv\Scripts\Activate.ps1
# Windows CMD:
.\.venv\Scripts\activate.bat
# Linux/Mac:
source .venv/bin/activate
```

**Option B: Using standard Python venv**
```bash
python -m venv .venv

# Activate (same as above)
# Windows PowerShell:
.\.venv\Scripts\Activate.ps1
# Linux/Mac:
source .venv/bin/activate
```

### Step 3: Install Dependencies

```bash
# Using UV (faster):
uv pip install -e .

# Or using pip:
pip install -e .
```

### Step 4: Configure Environment

```bash
# Create .env file from template
copy .env.example .env    # Windows
cp .env.example .env      # Linux/Mac

# Edit .env and add your OpenAI API key
# OPENAI_API_KEY=sk-your-key-here
```

### Step 5: Run the Application

```bash
streamlit run app.py
```

**Open your browser: http://localhost:8501**

### Quick Setup Script (Alternative)

Instead of manual steps, use the automated setup:

```bash
# Windows PowerShell:
.\setup.ps1

# Linux/Mac:
chmod +x setup.sh && ./setup.sh

# Cross-platform Python:
python setup.py
```

---

## üê≥ Quick Start - Docker Setup

### Prerequisites

- **Docker Desktop** ([Download](https://www.docker.com/products/docker-desktop/))
- **OpenAI API Key**

### Step 1: Clone the Repository

```bash
git clone https://github.com/ayush-baunthiyal/AIAgent-Chat-With-Doc.git
cd AIAgent-Chat-With-Doc
```

### Step 2: Create Environment File

```bash
# Create .env file
copy .env.example .env    # Windows
cp .env.example .env      # Linux/Mac

# Edit .env and add your OpenAI API key
# OPENAI_API_KEY=sk-your-key-here
```

### Step 3: Build Docker Image

```bash
docker build -t rag-chat-assistant .
```

### Step 4: Run Container

**Windows PowerShell:**
```powershell
docker run -d --name rag-chat -p 8501:8501 --env-file .env -v "${PWD}/data:/app/data" rag-chat-assistant
```

**Linux/Mac:**
```bash
docker run -d --name rag-chat -p 8501:8501 --env-file .env -v "$(pwd)/data:/app/data" rag-chat-assistant
```

**Open your browser: http://localhost:8501**

### Docker Commands Reference

| Command | Description |
|---------|-------------|
| `docker logs rag-chat` | View container logs |
| `docker logs -f rag-chat` | Follow logs in real-time |
| `docker stop rag-chat` | Stop the container |
| `docker start rag-chat` | Start stopped container |
| `docker rm rag-chat` | Remove container |
| `docker rm -f rag-chat` | Force remove running container |

### Rebuild After Code Changes

```bash
docker rm -f rag-chat
docker build -t rag-chat-assistant .
docker run -d --name rag-chat -p 8501:8501 --env-file .env -v "${PWD}/data:/app/data" rag-chat-assistant
```

---

## ‚öôÔ∏è Configuration

### Environment Variables (.env)

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | *required* | Your OpenAI API key |
| `OPENAI_MODEL` | `gpt-4-turbo-preview` | LLM model for responses |
| `OPENAI_EMBEDDING_MODEL` | `text-embedding-3-small` | Model for embeddings |
| `CHROMA_DB_PATH` | `./data/chroma_db` | Path to Chroma database |
| `CHROMA_COLLECTION_NAME` | `documents` | Chroma collection name |
| `CHUNK_SIZE` | `1000` | Characters per chunk |
| `CHUNK_OVERLAP` | `200` | Overlap between chunks |
| `TOP_K_CHUNKS` | `5` | Chunks to retrieve per query |
| `RELEVANCE_THRESHOLD` | `0.3` | Minimum relevance score (0-1) |
| `MAX_ITERATIONS` | `10` | Max ReAct reasoning loops |
| `TEMPERATURE` | `0.7` | LLM creativity (0-1) |
| `LOG_LEVEL` | `INFO` | Logging verbosity |

### Example .env File

```env
# Required
OPENAI_API_KEY=sk-your-api-key-here

# Optional - Model Configuration
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Optional - Retrieval Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K_CHUNKS=5
RELEVANCE_THRESHOLD=0.3

# Optional - Agent Configuration
MAX_ITERATIONS=10
TEMPERATURE=0.7
```

---

## üìÅ Project Structure

```
AIAgent-Chat-With-Doc/
‚îú‚îÄ‚îÄ app.py                      # Streamlit main application
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Pydantic settings management
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py   # PDF/text loading and chunking
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py           # OpenAI embedding wrapper
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py         # Chroma vector store operations
‚îÇ   ‚îú‚îÄ‚îÄ mcp_client.py           # MCP adapter for LangGraph
‚îÇ   ‚îú‚îÄ‚îÄ mcp_server/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server.py           # MCP server setup
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools.py            # Document search/retrieval tools
‚îÇ   ‚îú‚îÄ‚îÄ retrieval.py            # Hybrid retrieval service
‚îÇ   ‚îú‚îÄ‚îÄ agent.py                # LangGraph ReAct agent
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py              # System prompts and templates
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                # Helper functions
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ chroma_db/              # Chroma persistent storage
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_document_processor.py
‚îú‚îÄ‚îÄ pyproject.toml              # Project dependencies (UV/pip)
‚îú‚îÄ‚îÄ Dockerfile                  # Container definition
‚îú‚îÄ‚îÄ setup.py                    # Cross-platform setup script
‚îú‚îÄ‚îÄ setup.ps1                   # Windows PowerShell setup
‚îú‚îÄ‚îÄ setup.sh                    # Linux/Mac setup
‚îú‚îÄ‚îÄ .env.example                # Environment template
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

---

## üîç RAG/LLM Approach & Decisions

### LLM Selection

| Model | Use Case | Rationale |
|-------|----------|-----------|
| **GPT-4-turbo** | Primary LLM | Best reasoning for ReAct agent, high-quality responses |
| **GPT-3.5-turbo** | Cost alternative | Lower cost, faster, acceptable for simple queries |

### Embedding Strategy

- **Model**: `text-embedding-3-small` (1536 dimensions)
- **Why**: Cost-effective ($0.02/1M tokens), good semantic quality
- **Alternative**: `text-embedding-3-large` for higher accuracy at 2x cost

### Chunking Strategy

```python
RecursiveCharacterTextSplitter(
    chunk_size=1000,      # ~250 words per chunk
    chunk_overlap=200,    # 20% overlap for context
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

**Rationale**: Balances context preservation with retrieval granularity. Overlap prevents losing information at chunk boundaries.

### Retrieval Pipeline

1. **Query Embedding**: User query ‚Üí 1536-dim vector
2. **Similarity Search**: Chroma HNSW index ‚Üí top-k candidates
3. **Relevance Filtering**: Score threshold (>0.3) ‚Üí filtered results
4. **Context Assembly**: Concatenate chunks with metadata

### ReAct Agent Loop

```
1. THINK: Analyze query, plan approach
2. ACT: Call MCP tools (search_documents, get_chunk)
3. OBSERVE: Process tool results
4. REPEAT: Until sufficient context gathered
5. GENERATE: Synthesize final answer with citations
```

### Prompt Engineering

- **System Prompt**: ReAct instructions, tool usage guidelines
- **Context Prompt**: Retrieved chunks with source metadata
- **Response Prompt**: Citation format, answer structure

---

## üö¢ Production Considerations

### Current State (Local Deployment)

‚úÖ Local Docker Desktop deployment  
‚úÖ Persistent Chroma storage  
‚úÖ Environment-based configuration  
‚úÖ Basic logging and error handling  
‚úÖ Session-based conversation memory  
‚úÖ Context length management (message trimming)  

### Scaling for Production

| Area | Local | Production |
|------|-------|------------|
| **Vector DB** | Chroma (local file) | Pinecone, Weaviate, Qdrant (cloud) |
| **LLM** | OpenAI API | Azure OpenAI, self-hosted LLM |
| **Memory** | Session (in-memory) | Redis, PostgreSQL |
| **Orchestration** | Single container | Kubernetes, ECS |
| **Monitoring** | Console logs | Datadog, New Relic, LangSmith |
| **Auth** | None | OAuth, API keys |

### Security Checklist

- [ ] API key management (secrets manager)
- [ ] Input sanitization (prompt injection prevention)
- [ ] Rate limiting per user
- [ ] Data encryption at rest
- [ ] HTTPS termination
- [ ] Authentication/authorization

---

## üß™ Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test file
pytest tests/test_document_processor.py -v
```

---

## üîß Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| **PowerShell script disabled** | Run: `Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass` |
| **Python 3.14+ errors** | Use Python 3.10-3.13 (onnxruntime compatibility) |
| **Docker context errors** | Run: `docker context use desktop-linux` |
| **localhost:8501 not loading** | Wait 10 seconds for Streamlit startup |
| **"Checkpointer requires thread_id"** | Update to latest code (MemorySaver removed) |
| **Context length exceeded** | Conversation is auto-trimmed (last 10 messages) |
| **No results found** | Lower `RELEVANCE_THRESHOLD` in .env |

### Logs

**Local:**
```bash
# Check Streamlit output in terminal
streamlit run app.py
```

**Docker:**
```bash
docker logs rag-chat
docker logs -f rag-chat  # Follow logs
```

---

## ü§ñ AI Tool Usage

### How AI Was Used

- ‚úÖ Boilerplate code generation
- ‚úÖ Documentation templates
- ‚úÖ LangGraph/MCP integration patterns
- ‚úÖ Debugging assistance

### Best Practices

**Do:**
- Review all AI-generated code
- Test thoroughly
- Understand the code you're using

**Don't:**
- Blindly accept suggestions
- Skip testing
- Use AI for critical security code without review

---

## üìù License

MIT License - Feel free to use and modify.

---

## üë§ Author

**Ayush Baunthiyal**  
Staff Software Engineer / AI Engineer

---

## üîÆ Future Enhancements

1. **Streaming responses** - Token-by-token display
2. **Multi-document reasoning** - Cross-document queries
3. **Persistent memory** - SQLite conversation history
4. **Document management** - Delete/update specific documents
5. **Export functionality** - Save conversations
6. **Quality metrics** - RAG evaluation scoring
7. **User feedback** - Thumbs up/down for improvement
